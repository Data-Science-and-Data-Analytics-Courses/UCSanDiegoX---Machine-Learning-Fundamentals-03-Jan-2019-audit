{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "winery-classification-bivariate.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Data-Science-and-Data-Analytics-Courses/UCSanDiegoX---Machine-Learning-Fundamentals-03-Jan-2019-audit/blob/master/Week%2002%20Generative%20Modeling%20I/winery-bivariate/winery-classification-bivariate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "NQDi503t8vgE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Winery classification with the bivariate Gaussian\n",
        "\n",
        "Our first generative model for Winery classification used just one feature. Now we use two features, modeling each class by a **bivariate Gaussian**."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UdDQ4H0VmPpJ"
      },
      "cell_type": "markdown",
      "source": [
        "# Clone remote"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cXqTKogHXEr9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "f9404088-852f-43fc-f86a-3f57d05956d8"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "URL = \"https://github.com/Data-Science-and-Data-Analytics-Courses/UCSanDiegoX---Machine-Learning-Fundamentals-03-Jan-2019-audit\"\n",
        "NBDIR = \"Week 02 Generative Modeling I/winery-bivariate\"\n",
        "\n",
        "def clone(url, dpath=\".\", branch=\"master\"):\n",
        "  \"\"\"\n",
        "  Clone remote branch from url into dpath\n",
        "  branch not provided: clone all branches\n",
        "  \"\"\"\n",
        "\n",
        "  url = url.strip(\"/\")\n",
        "  rname = os.path.basename(url)\n",
        "  rpath = os.path.join(dpath, rname)\n",
        "\n",
        "  # Raise error if dpath inside existing repository\n",
        "  is_out = !git -C \"$dpath\" rev-parse\n",
        "  if not is_out: # inside repository\n",
        "    raise ValueError(\"Can't clone into existing repository\")\n",
        "  \n",
        "  # Clone specific branch\n",
        "  if branch:\n",
        "    !git clone --single-branch --branch \"$branch\" \"$url\" \"$rpath\"\n",
        "  # Clone all branches\n",
        "  else:\n",
        "    !git clone \"$url\" \"$rpath\"\n",
        "  os.chdir(rpath)\n",
        "  \n",
        "  bname = !git rev-parse --abbrev-ref HEAD\n",
        "  print(\"Current\")\n",
        "  print(\"{branch}\\t{directory}\".format(branch=bname, directory=os.getcwd()))\n",
        "  \n",
        "clone(URL)\n",
        "%run .Importable.ipynb\n",
        "%cd \"$NBDIR\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into './UCSanDiegoX---Machine-Learning-Fundamentals-03-Jan-2019-audit'...\n",
            "remote: Enumerating objects: 123, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
            "remote: Total 234 (delta 59), reused 5 (delta 1), pack-reused 111\u001b[K\n",
            "Receiving objects: 100% (234/234), 2.55 MiB | 2.38 MiB/s, done.\n",
            "Resolving deltas: 100% (94/94), done.\n",
            "Current\n",
            "['master']\t/content/UCSanDiegoX---Machine-Learning-Fundamentals-03-Jan-2019-audit\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "<style type='text/css'>\n",
              ".hll { background-color: #ffffcc }\n",
              ".c { color: #408080; font-style: italic } /* Comment */\n",
              ".err { border: 1px solid #FF0000 } /* Error */\n",
              ".k { color: #008000; font-weight: bold } /* Keyword */\n",
              ".o { color: #666666 } /* Operator */\n",
              ".ch { color: #408080; font-style: italic } /* Comment.Hashbang */\n",
              ".cm { color: #408080; font-style: italic } /* Comment.Multiline */\n",
              ".cp { color: #BC7A00 } /* Comment.Preproc */\n",
              ".cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */\n",
              ".c1 { color: #408080; font-style: italic } /* Comment.Single */\n",
              ".cs { color: #408080; font-style: italic } /* Comment.Special */\n",
              ".gd { color: #A00000 } /* Generic.Deleted */\n",
              ".ge { font-style: italic } /* Generic.Emph */\n",
              ".gr { color: #FF0000 } /* Generic.Error */\n",
              ".gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
              ".gi { color: #00A000 } /* Generic.Inserted */\n",
              ".go { color: #888888 } /* Generic.Output */\n",
              ".gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
              ".gs { font-weight: bold } /* Generic.Strong */\n",
              ".gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
              ".gt { color: #0044DD } /* Generic.Traceback */\n",
              ".kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
              ".kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
              ".kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
              ".kp { color: #008000 } /* Keyword.Pseudo */\n",
              ".kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
              ".kt { color: #B00040 } /* Keyword.Type */\n",
              ".m { color: #666666 } /* Literal.Number */\n",
              ".s { color: #BA2121 } /* Literal.String */\n",
              ".na { color: #7D9029 } /* Name.Attribute */\n",
              ".nb { color: #008000 } /* Name.Builtin */\n",
              ".nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
              ".no { color: #880000 } /* Name.Constant */\n",
              ".nd { color: #AA22FF } /* Name.Decorator */\n",
              ".ni { color: #999999; font-weight: bold } /* Name.Entity */\n",
              ".ne { color: #D2413A; font-weight: bold } /* Name.Exception */\n",
              ".nf { color: #0000FF } /* Name.Function */\n",
              ".nl { color: #A0A000 } /* Name.Label */\n",
              ".nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
              ".nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
              ".nv { color: #19177C } /* Name.Variable */\n",
              ".ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
              ".w { color: #bbbbbb } /* Text.Whitespace */\n",
              ".mb { color: #666666 } /* Literal.Number.Bin */\n",
              ".mf { color: #666666 } /* Literal.Number.Float */\n",
              ".mh { color: #666666 } /* Literal.Number.Hex */\n",
              ".mi { color: #666666 } /* Literal.Number.Integer */\n",
              ".mo { color: #666666 } /* Literal.Number.Oct */\n",
              ".sb { color: #BA2121 } /* Literal.String.Backtick */\n",
              ".sc { color: #BA2121 } /* Literal.String.Char */\n",
              ".sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
              ".s2 { color: #BA2121 } /* Literal.String.Double */\n",
              ".se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */\n",
              ".sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
              ".si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */\n",
              ".sx { color: #008000 } /* Literal.String.Other */\n",
              ".sr { color: #BB6688 } /* Literal.String.Regex */\n",
              ".s1 { color: #BA2121 } /* Literal.String.Single */\n",
              ".ss { color: #19177C } /* Literal.String.Symbol */\n",
              ".bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
              ".vc { color: #19177C } /* Name.Variable.Class */\n",
              ".vg { color: #19177C } /* Name.Variable.Global */\n",
              ".vi { color: #19177C } /* Name.Variable.Instance */\n",
              ".il { color: #666666 } /* Literal.Number.Integer.Long */\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/content/UCSanDiegoX---Machine-Learning-Fundamentals-03-Jan-2019-audit/Week 02 Generative Modeling I/winery-bivariate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VleCUuXy8vgH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Load in the data set"
      ]
    },
    {
      "metadata": {
        "id": "LMY0Crcu8vgJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As in the univariate case, we start by loading in the Wine data set. Make sure the file `wine.data.txt` is in the same directory as this notebook.\n",
        "\n",
        "Recall that there are 178 data points, each with 13 features and a label (1,2,3). As before, we will divide this into a training set of 130 points and a test set of 48 points."
      ]
    },
    {
      "metadata": {
        "id": "1l5bORwV8vgK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Standard includes\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# Useful module for dealing with the Gaussian density\n",
        "from scipy.stats import norm, multivariate_normal \n",
        "# installing packages for interactive graphs\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gbCidUXZ8vgO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load data set.\n",
        "data = np.loadtxt('wine.data.txt', delimiter=',')\n",
        "# Names of features\n",
        "featurenames = ['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash','Magnesium', 'Total phenols', \n",
        "                'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', \n",
        "                'OD280/OD315 of diluted wines', 'Proline']\n",
        "# Split 178 instances into training set (trainx, trainy) of size 130 and test set (testx, testy) of size 48\n",
        "np.random.seed(0)\n",
        "perm = np.random.permutation(178)\n",
        "trainx = data[perm[0:130],1:14]\n",
        "trainy = data[perm[0:130],0]\n",
        "testx = data[perm[130:178], 1:14]\n",
        "testy = data[perm[130:178],0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nUFGKv0R8vgR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Look at the distribution of two features from one of the wineries"
      ]
    },
    {
      "metadata": {
        "id": "FyKLgaq18vgR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our goal is to plot the distribution of two features from a particular winery. We will use several helper functions for this. It is worth understanding each of these."
      ]
    },
    {
      "metadata": {
        "id": "7q9reenA8vgS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first helper function fits a Gaussian to a data set, restricting attention to specified features.\n",
        "It returns the mean and covariance matrix of the Gaussian."
      ]
    },
    {
      "metadata": {
        "id": "HWbtVLzK8vgT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Fit a Gaussian to a data set using the selected features\n",
        "def fit_gaussian(x, features):\n",
        "    mu = np.mean(x[:,features], axis=0)\n",
        "    covar = np.cov(x[:,features], rowvar=0, bias=1)\n",
        "    return mu, covar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wUhGJJ2m8vgV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For example, let's look at the Gaussian we get for winery 1, using features 0 ('alcohol') and 6 ('flavanoids')."
      ]
    },
    {
      "metadata": {
        "id": "NCja8zcE8vgX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f1 = 0\n",
        "f2 = 6\n",
        "label = 1\n",
        "mu, covar = fit_gaussian(trainx[trainy==label,:], [f1,f2])\n",
        "print \"Mean:\\n\" + str(mu)\n",
        "print \"Covariance matrix:\\n\" + str(covar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DNXBT1Gm8vgZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we will construct a routine for displaying points sampled from a two-dimensional Gaussian, as well as a few contour lines. Part of doing this involves deciding what range to use for each axis. We begin with a little helper function that takes as input an array of numbers (values along a single feature) and returns the range in which these numbers lie."
      ]
    },
    {
      "metadata": {
        "id": "983Yzg3D8vga",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Find the range within which an array of numbers lie, with a little buffer\n",
        "def find_range(x):\n",
        "    lower = min(x)\n",
        "    upper = max(x)\n",
        "    width = upper - lower\n",
        "    lower = lower - 0.2 * width\n",
        "    upper = upper + 0.2 * width\n",
        "    return lower, upper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fwFWFLNT8vgc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we define a routine that plots a few contour lines of a given two-dimensional Gaussian.\n",
        "It takes as input:\n",
        "* `mu`, `cov`: the parameters of the Gaussian\n",
        "* `x1g`, `x2g`: the grid (along the two axes) at which the density is to be computed\n",
        "* `col`: the color of the contour lines"
      ]
    },
    {
      "metadata": {
        "id": "b3GYhjDE8vgd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_contours(mu, cov, x1g, x2g, col):\n",
        "    rv = multivariate_normal(mean=mu, cov=cov)\n",
        "    z = np.zeros((len(x1g),len(x2g)))\n",
        "    for i in range(0,len(x1g)):\n",
        "        for j in range(0,len(x2g)):\n",
        "            z[j,i] = rv.logpdf([x1g[i], x2g[j]]) \n",
        "    sign, logdet = np.linalg.slogdet(cov)\n",
        "    normalizer = -0.5 * (2 * np.log(6.28) + sign * logdet)\n",
        "    for offset in range(1,4):\n",
        "        plt.contour(x1g,x2g,z, levels=[normalizer - offset], colors=col, linewidths=2.0, linestyles='solid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UHqMX9ie8vgg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The function **two_features_plot** takes an input two features and a label, and displays the distribution for the specified winery and pair of features.\n",
        "\n",
        "The first line allows you to specify the parameters interactively using sliders."
      ]
    },
    {
      "metadata": {
        "id": "Z5cY4vZi8vgh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "@interact_manual( f1=IntSlider(0,0,12,1), f2=IntSlider(6,0,12,1), label=IntSlider(1,1,3,1) )\n",
        "def two_features_plot(f1,f2,label):\n",
        "    if f1 == f2: # we need f1 != f2\n",
        "        print \"Please choose different features for f1 and f2.\"\n",
        "        return  \n",
        "    \n",
        "    # Set up plot\n",
        "    x1_lower, x1_upper = find_range(trainx[trainy==label,f1])\n",
        "    x2_lower, x2_upper = find_range(trainx[trainy==label,f2])\n",
        "    plt.xlim(x1_lower, x1_upper) # limit along x1-axis\n",
        "    plt.ylim(x2_lower, x2_upper) # limit along x2-axis\n",
        "    \n",
        "    # Plot the training points along the two selected features\n",
        "    plt.plot(trainx[trainy==label, f1], trainx[trainy==label, f2], 'ro')\n",
        "\n",
        "    # Define a grid along each axis; the density will be computed at each grid point\n",
        "    res = 200 # resolution\n",
        "    x1g = np.linspace(x1_lower, x1_upper, res)\n",
        "    x2g = np.linspace(x2_lower, x2_upper, res)\n",
        "\n",
        "    # Now plot a few contour lines of the density\n",
        "    mu, cov = fit_gaussian(trainx[trainy==label,:], [f1,f2])\n",
        "    plot_contours(mu, cov, x1g, x2g, 'k')\n",
        "    \n",
        "    # Finally, display\n",
        "    plt.xlabel(featurenames[f1], fontsize=14, color='red')\n",
        "    plt.ylabel(featurenames[f2], fontsize=14, color='red')\n",
        "    plt.title('Class ' + str(label), fontsize=14, color='blue')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M183PLrZ8vgk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Fit a Gaussian to each class"
      ]
    },
    {
      "metadata": {
        "id": "7GIjuuQR8vgl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now define a function that will fit a Gaussian generative model to the three classes, restricted to a given list of features. The function returns:\n",
        "* `mu`: the means of the Gaussians, one per row\n",
        "* `covar`: covariance matrices of each of the Gaussians\n",
        "* `pi`: list of three class weights summing to 1"
      ]
    },
    {
      "metadata": {
        "id": "2IWL9oIQ8vgo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Assumes y takes on values 1,2,3\n",
        "def fit_generative_model(x, y, features):\n",
        "    k = 3 # number of classes\n",
        "    d = len(features) # number of features\n",
        "    mu = np.zeros((k+1,d)) # list of means\n",
        "    covar = np.zeros((k+1,d,d)) # list of covariance matrices\n",
        "    pi = np.zeros(k+1) # list of class weights\n",
        "    for label in range(1,k+1):\n",
        "        indices = (y==label)\n",
        "        mu[label,:], covar[label,:,:] = fit_gaussian(x[indices,:], features)\n",
        "        pi[label] = float(sum(indices))/float(len(y))\n",
        "    return mu, covar, pi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ne5xcHP48vgq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we will plot the three Gaussians."
      ]
    },
    {
      "metadata": {
        "id": "4SuliZCD8vgq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "@interact_manual( f1=IntSlider(0,0,12,1), f2=IntSlider(6,0,12,1) )\n",
        "def three_class_plot(f1,f2):\n",
        "    if f1 == f2: # we need f1 != f2\n",
        "        print \"Please choose different features for f1 and f2.\"\n",
        "        return  \n",
        "    \n",
        "    # Set up plot\n",
        "    x1_lower, x1_upper = find_range(trainx[:,f1])\n",
        "    x2_lower, x2_upper = find_range(trainx[:,f2])\n",
        "    plt.xlim(x1_lower, x1_upper) # limit along x1-axis\n",
        "    plt.ylim(x2_lower, x2_upper) # limit along x2-axis\n",
        "    \n",
        "    # Plot the training points along the two selected features\n",
        "    colors = ['r', 'k', 'g']\n",
        "    for label in range(1,4):\n",
        "        plt.plot(trainx[trainy==label,f1], trainx[trainy==label,f2], marker='o', ls='None', c=colors[label-1])\n",
        "\n",
        "    # Define a grid along each axis; the density will be computed at each grid point\n",
        "    res = 200 # resolution\n",
        "    x1g = np.linspace(x1_lower, x1_upper, res)\n",
        "    x2g = np.linspace(x2_lower, x2_upper, res)\n",
        "\n",
        "    # Show the Gaussian fit to each class, using features f1,f2\n",
        "    mu, covar, pi = fit_generative_model(trainx, trainy, [f1,f2])\n",
        "    for label in range(1,4):\n",
        "        gmean = mu[label,:]\n",
        "        gcov = covar[label,:,:]\n",
        "        plot_contours(gmean, gcov, x1g, x2g, colors[label-1])\n",
        "\n",
        "    # Finally, display\n",
        "    plt.xlabel(featurenames[f1], fontsize=14, color='red')\n",
        "    plt.ylabel(featurenames[f2], fontsize=14, color='red')\n",
        "    plt.title('Wine data', fontsize=14, color='blue')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OdHtysbZ8vgs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Predict labels for the test points"
      ]
    },
    {
      "metadata": {
        "id": "8_iVhBtM8vgt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How well we can predict the class (1,2,3) based just on these two features?\n",
        "\n",
        "We start with a testing procedure that is analogous to what we developed in the 1-d case."
      ]
    },
    {
      "metadata": {
        "id": "l7nVzA7J8vgu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Now test the performance of a predictor based on a subset of features\n",
        "@interact( f1=IntSlider(0,0,12,1), f2=IntSlider(6,0,12,1) )\n",
        "def test_model(f1, f2):\n",
        "    if f1 == f2: # need f1 != f2\n",
        "        print \"Please choose different features for f1 and f2.\"\n",
        "        return  \n",
        "    features= [f1,f2]\n",
        "    mu, covar, pi = fit_generative_model(trainx, trainy, features)\n",
        "    \n",
        "    k = 3 # Labels 1,2,...,k\n",
        "    nt = len(testy) # Number of test points\n",
        "    score = np.zeros((nt,k+1))\n",
        "    for i in range(0,nt):\n",
        "        for label in range(1,k+1):\n",
        "            score[i,label] = np.log(pi[label]) + \\\n",
        "            multivariate_normal.logpdf(testx[i,features], mean=mu[label,:], cov=covar[label,:,:])\n",
        "    predictions = np.argmax(score[:,1:4], axis=1) + 1\n",
        "    # Finally, tally up score\n",
        "    errors = np.sum(predictions != testy)\n",
        "    print \"Test error using feature(s): \",\n",
        "    for f in features:\n",
        "        print \"'\" + featurenames[f] + \"'\" + \" \",\n",
        "    print\n",
        "    print \"Errors: \" + str(errors) + \"/\" + str(nt)# Now test the performance of a predictor based on a subset of features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KqqGCUgk8vgw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### <font color=\"magenta\">Fast exercise 1</font>"
      ]
    },
    {
      "metadata": {
        "id": "Yilc92Sz8vgx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Different pairs of features yield different test errors.\n",
        "* What is the smallest achievable test error?\n",
        "* Which pair of features achieves this minimum test error?\n",
        "\n",
        "*Make a note of your answers to these questions, as you will need to enter them as part of this week's assignment.*"
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "sVXphO0A8vgy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5. The decision boundary "
      ]
    },
    {
      "metadata": {
        "id": "rMzLY_Aa8vgz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The function **show_decision_boundary** takes as input two features, builds a classifier based only on these two features, and shows a plot that contains both the training data and the decision boundary.\n",
        "\n",
        "To compute the decision boundary, a dense grid is defined on the two-dimensional input space and the classifier is applied to every grid point. The built-in `pyplot.contour` function can then be invoked to depict the boundary."
      ]
    },
    {
      "metadata": {
        "id": "0R-qt_bL8vgz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_decision_boundary(f1,f2):\n",
        "    # Fit Gaussian to each class\n",
        "    mu, covar, pi = fit_generative_model(trainx, trainy, [f1,f2])\n",
        "    \n",
        "    # Set up dimensions of plot\n",
        "    x1_lower, x1_upper = find_range(trainx[:,f1])\n",
        "    x2_lower, x2_upper = find_range(trainx[:,f2])\n",
        "    plt.xlim([x1_lower,x1_upper])\n",
        "    plt.ylim([x2_lower,x2_upper])\n",
        "\n",
        "    # Plot points in training set\n",
        "    colors = ['r', 'k', 'g']\n",
        "    for label in range(1,4):\n",
        "        plt.plot(trainx[trainy==label,f1], trainx[trainy==label,f2], marker='o', ls='None', c=colors[label-1])\n",
        "\n",
        "    # Define a dense grid; every point in the grid will be classified according to the generative model\n",
        "    res = 200\n",
        "    x1g = np.linspace(x1_lower, x1_upper, res)\n",
        "    x2g = np.linspace(x2_lower, x2_upper, res)\n",
        "\n",
        "    # Declare random variables corresponding to each class density\n",
        "    random_vars = {}\n",
        "    for label in range(1,4):\n",
        "        random_vars[label] = multivariate_normal(mean=mu[label,:],cov=covar[label,:,:])\n",
        "\n",
        "    # Classify every point in the grid; these are stored in an array Z[]\n",
        "    Z = np.zeros((len(x1g), len(x2g)))\n",
        "    for i in range(0,len(x1g)):\n",
        "        for j in range(0,len(x2g)):\n",
        "            scores = []\n",
        "            for label in range(1,4):\n",
        "                scores.append(np.log(pi[label]) + random_vars[label].logpdf([x1g[i],x2g[j]]))\n",
        "            Z[i,j] = np.argmax(scores) + 1\n",
        "\n",
        "    # Plot the contour lines\n",
        "    plt.contour(x1g,x2g,Z.T,3,cmap='seismic')\n",
        "    \n",
        "    # Finally, show the image\n",
        "    plt.xlabel(featurenames[f1], fontsize=14, color='red')\n",
        "    plt.ylabel(featurenames[f2], fontsize=14, color='red')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XvaMhBPZ8vg2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's use the function above to draw the decision boundary using features 0 ('alcohol') and 6 ('flavanoids')."
      ]
    },
    {
      "metadata": {
        "id": "xw8OPEYq8vg3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "show_decision_boundary(0,6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eP4YxzDo8vg7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### <font color=\"magenta\">Fast exercise 2</font>"
      ]
    },
    {
      "metadata": {
        "id": "CTZ4h7x18vg8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Can you add interactive sliders to function **show_decision_boundary**?"
      ]
    },
    {
      "metadata": {
        "id": "-nxueN8G8vg9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### <font color=\"magenta\">Fast exercise 3</font>"
      ]
    },
    {
      "metadata": {
        "id": "O48CG06x8vg9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Produce a plot similar to that of **show_decision_boundary**, but in which just the **test** data is shown.\n",
        "Look back at your answer to *Fast exercise 1*. Is it corroborated by your plot? Are the errors clearly visible?"
      ]
    }
  ]
}
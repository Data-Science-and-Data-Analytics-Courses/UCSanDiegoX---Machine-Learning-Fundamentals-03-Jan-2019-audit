{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment-svm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Data-Science-and-Data-Analytics-Courses/UCSanDiegoX---Machine-Learning-Fundamentals-03-Jan-2019-audit/blob/master/Week%2006%20Linear%20Classification/sentiment_svm/sentiment-svm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "CigeLnu0P0hD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis with support vector machines\n",
        "\n",
        "In this notebook, we will revisit a learning task that we encountered earlier in the course: predicting the *sentiment* (positive or negative) of a single sentence taken from a review of a movie, restaurant, or product. The data set consists of 3000 labeled sentences, which we divide into a training set of size 2500 and a test set of size 500. Previously we found a logistic regression classifier. Today we will use a support vector machine.\n",
        "\n",
        "Before starting on this notebook, make sure the folder `sentiment_labelled_sentences` (containing the data file `full_set.txt`) is in the same directory. Recall that the data can be downloaded from https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences. "
      ]
    },
    {
      "metadata": {
        "id": "D9lDxD0mP7uS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Clone remote"
      ]
    },
    {
      "metadata": {
        "id": "PIG0_CZdP73I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "URL = \"https://github.com/Data-Science-and-Data-Analytics-Courses/UCSanDiegoX---Machine-Learning-Fundamentals-03-Jan-2019-audit\"\n",
        "NBDIR = \"Week 06 Linear Classification/sentiment_svm\"\n",
        "\n",
        "def clone(url, dest=\".\", branch=\"master\", reloc=True):\n",
        "  \"\"\"\n",
        "  Clone remote branch from url into dest\n",
        "  branch not provided: clone all branches\n",
        "  reloc is True: relocate to repository\n",
        "  \"\"\"\n",
        "\n",
        "  url = url.strip(\" /\")\n",
        "  repo = Path(dest, os.path.basename(url)).resolve()\n",
        "\n",
        "  # dest must not be inside existing repository\n",
        "  is_out = !git -C \"$dest\" rev-parse\n",
        "  if not is_out: # inside repository\n",
        "    raise ValueError(\"Can't clone into existing repository\")\n",
        "  \n",
        "  # Clone\n",
        "  p = repo.as_posix()\n",
        "  if branch: # specific branch\n",
        "    !git clone --single-branch \"$url\" -b \"$branch\" \"$p\"\n",
        "  else: # all branches\n",
        "    !git clone \"$url\" \"$p\"\n",
        "  \n",
        "  # Relocate\n",
        "  if reloc:\n",
        "    %cd \"$repo\"\n",
        "\n",
        "  return repo.as_posix()\n",
        "\n",
        "REPO = clone(URL)\n",
        "%run .Importable.ipynb\n",
        "sys.path.append(REPO)\n",
        "%cd \"$NBDIR\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WF3W2CB3P0hH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Loading and preprocessing the data\n",
        " \n",
        "Here we follow exactly the same steps as we did earlier."
      ]
    },
    {
      "metadata": {
        "id": "BR8PaPv9P0hJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import string\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.rc('xtick', labelsize=14) \n",
        "matplotlib.rc('ytick', labelsize=14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E7DYOaKBP0hM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "## Read in the data set.\n",
        "with open(\"sentiment_labelled_sentences/full_set.txt\") as f:\n",
        "    content = f.readlines()\n",
        "    \n",
        "## Remove leading and trailing white space\n",
        "content = [x.strip() for x in content]\n",
        "\n",
        "## Separate the sentences from the labels\n",
        "sentences = [x.split(\"\\t\")[0] for x in content]\n",
        "labels = [x.split(\"\\t\")[1] for x in content]\n",
        "\n",
        "## Transform the labels from '0 v.s. 1' to '-1 v.s. 1'\n",
        "y = np.array(labels, dtype='int8')\n",
        "y = 2*y - 1\n",
        "\n",
        "## Read in the data set.\n",
        "with open(\"sentiment_labelled_sentences/full_set.txt\") as f:\n",
        "    content = f.readlines()\n",
        "    \n",
        "## Remove leading and trailing white space\n",
        "content = [x.strip() for x in content]\n",
        "\n",
        "## Separate the sentences from the labels\n",
        "sentences = [x.split(\"\\t\")[0] for x in content]\n",
        "labels = [x.split(\"\\t\")[1] for x in content]\n",
        "\n",
        "## Transform the labels from '0 v.s. 1' to '-1 v.s. 1'\n",
        "y = np.array(labels, dtype='int8')\n",
        "y = 2*y - 1\n",
        "\n",
        "## full_remove takes a string x and a list of characters removal_list \n",
        "## returns x with all the characters in removal_list replaced by ' '\n",
        "def full_remove(x, removal_list):\n",
        "    for w in removal_list:\n",
        "        x = x.replace(w, ' ')\n",
        "    return x\n",
        "\n",
        "## Remove digits\n",
        "digits = [str(x) for x in range(10)]\n",
        "digit_less = [full_remove(x, digits) for x in sentences]\n",
        "\n",
        "## Remove punctuation\n",
        "punc_less = [full_remove(x, list(string.punctuation)) for x in digit_less]\n",
        "\n",
        "## Make everything lower-case\n",
        "sents_lower = [x.lower() for x in punc_less]\n",
        "\n",
        "## Define our stop words\n",
        "stop_set = set(['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from'])\n",
        "\n",
        "## Remove stop words\n",
        "sents_split = [x.split() for x in sents_lower]\n",
        "sents_processed = [\" \".join(list(filter(lambda a: a not in stop_set, x))) for x in sents_split]\n",
        "\n",
        "## Transform to bag of words representation.\n",
        "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 4500)\n",
        "data_features = vectorizer.fit_transform(sents_processed)\n",
        "\n",
        "## Append '1' to the end of each vector.\n",
        "data_mat = data_features.toarray()\n",
        "\n",
        "## Split the data into testing and training sets\n",
        "np.random.seed(0)\n",
        "test_inds = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
        "train_inds = list(set(range(len(labels))) - set(test_inds))\n",
        "\n",
        "train_data = data_mat[train_inds,]\n",
        "train_labels = y[train_inds]\n",
        "\n",
        "test_data = data_mat[test_inds,]\n",
        "test_labels = y[test_inds]\n",
        "\n",
        "print(\"train data: \", train_data.shape)\n",
        "print(\"test data: \", test_data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BjHlWWEWP0hQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Fitting a support vector machine to the data\n",
        "\n",
        "In support vector machines, we are given a set of examples $(x_1, y_1), \\ldots, (x_n, y_n)$ and we want to find a weight vector $w \\in \\mathbb{R}^d$ that solves the following optimization problem:\n",
        "\n",
        "$$ \\min_{w \\in \\mathbb{R}^d} \\| w \\|^2 + C \\sum_{i=1}^n \\xi_i $$\n",
        "$$ \\text{subject to } y_i \\langle w, x_i \\rangle \\geq 1 - \\xi_i \\text{ for all } i=1,\\ldots, n$$\n",
        "\n",
        "`scikit-learn` provides an SVM solver that we will use. The following routine takes as input the constant `C` (from the above optimization problem) and returns the training and test error of the resulting SVM model. It is invoked as follows:\n",
        "\n",
        "* `training_error, test_error = fit_classifier(C)`\n",
        "\n",
        "The default value for parameter `C` is 1.0."
      ]
    },
    {
      "metadata": {
        "id": "uDjLe1A2P0hT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "def fit_classifier(C_value=1.0):\n",
        "    clf = svm.LinearSVC(C=C_value, loss='hinge')\n",
        "    clf.fit(train_data,train_labels)\n",
        "    ## Get predictions on training data\n",
        "    train_preds = clf.predict(train_data)\n",
        "    train_error = float(np.sum((train_preds > 0.0) != (train_labels > 0.0)))/len(train_labels)\n",
        "    ## Get predictions on test data\n",
        "    test_preds = clf.predict(test_data)\n",
        "    test_error = float(np.sum((test_preds > 0.0) != (test_labels > 0.0)))/len(test_labels)\n",
        "    ##\n",
        "    return train_error, test_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B8rpdlccP0hX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cvals = [0.01,0.1,1.0,10.0,100.0,1000.0,10000.0]\n",
        "for c in cvals:\n",
        "    train_error, test_error = fit_classifier(c)\n",
        "    print (\"Error rate for C = %0.2f: train %0.3f test %0.3f\" % (c, train_error, test_error))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W2Ip_MXhP0ha",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Evaluating C by k-fold cross-validation\n",
        "\n",
        "As we can see, the choice of `C` has a very significant effect on the performance of the SVM classifier. We were able to assess this because we have a separate test set. In general, however, this is a luxury we won't possess. How can we choose `C` based only on the training set?\n",
        "\n",
        "A reasonable way to estimate the error associated with a specific value of `C` is by **`k-fold cross validation`**:\n",
        "* Partition the training set `S` into `k` equal-sized sized subsets `S_1, S_2, ..., S_k`.\n",
        "* For `i=1,2,...,k`, train a classifier with parameter `C` on `S - S_i` (all the training data except `S_i`) and test it on `S_i` to get error estimate `e_i`.\n",
        "* Average the errors: `(e_1 + ... + e_k)/k`\n",
        "\n",
        "The following procedure, **cross_validation_error**, does exactly this. It takes as input:\n",
        "* the training set `x,y`\n",
        "* the value of `C` to be evaluated\n",
        "* the integer `k`\n",
        "\n",
        "and it returns the estimated error of the classifier for that particular setting of `C`. <font color=\"magenta\">Look over the code carefully to understand exactly what it is doing.</font>"
      ]
    },
    {
      "metadata": {
        "id": "s8Ay2ZhpP0hb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cross_validation_error(x,y,C_value,k):\n",
        "    n = len(y)\n",
        "    ## Randomly shuffle indices\n",
        "    indices = np.random.permutation(n)\n",
        "    \n",
        "    ## Initialize error\n",
        "    err = 0.0\n",
        "    \n",
        "    ## Iterate over partitions\n",
        "    for i in range(k):\n",
        "        ## Partition indices\n",
        "        test_indices = indices[int(i*(n/k)):int((i+1)*(n/k) - 1)]\n",
        "        train_indices = np.setdiff1d(indices, test_indices)\n",
        "        \n",
        "        ## Train classifier with parameter c\n",
        "        clf = svm.LinearSVC(C=C_value, loss='hinge')\n",
        "        clf.fit(x[train_indices], y[train_indices])\n",
        "        \n",
        "        ## Get predictions on test partition\n",
        "        preds = clf.predict(x[test_indices])\n",
        "        \n",
        "        ## Compute error\n",
        "        err += float(np.sum((preds > 0.0) != (y[test_indices] > 0.0)))/len(test_indices)\n",
        "        \n",
        "    return err/k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gtFJggETP0hf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Picking a value of C"
      ]
    },
    {
      "metadata": {
        "id": "BG604GZOP0hf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The procedure **cross_validation_error** (above) evaluates a single candidate value of `C`. We need to use it repeatedly to identify a good `C`. \n",
        "\n",
        "<font color=\"magenta\">**For you to do:**</font> Write a function to choose `C`. It will be invoked as follows:\n",
        "\n",
        "* `c, err = choose_parameter(x,y,k)`\n",
        "\n",
        "where\n",
        "* `x,y` is the training data\n",
        "* `k` is the number of folds of cross-validation\n",
        "* `c` is chosen value of the parameter `C`\n",
        "* `err` is the cross-validation error estimate at `c`\n",
        "\n",
        "<font color=\"magenta\">Note:</font> This is a tricky business because a priori, even the order of magnitude of `C` is unknown. Should it be 0.0001 or 10000? You might want to think about trying multiple values that are arranged in a geometric progression (such as powers of ten). *In addition to returning a specific value of `C`, your function should **plot** the cross-validation errors for all the values of `C` it tried out (possibly using a log-scale for the `C`-axis).*"
      ]
    },
    {
      "metadata": {
        "id": "By_rYc93P0hi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def choose_parameter(x,y,k):\n",
        "    ### Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b2iR9WGDP0hl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's try out your routine!"
      ]
    },
    {
      "metadata": {
        "id": "5CYtTNqsP0ho",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "c, err = choose_parameter(train_data, train_labels, 10)\n",
        "print(\"Choice of C: \", c)\n",
        "print(\"Cross-validation error estimate: \", err)\n",
        "## Train it and test it\n",
        "clf = svm.LinearSVC(C=c, loss='hinge')\n",
        "clf.fit(train_data, train_labels)\n",
        "preds = clf.predict(test_data)\n",
        "error = float(np.sum((preds > 0.0) != (test_labels > 0.0)))/len(test_labels)\n",
        "print(\"Test error: \", error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QB3i5HHTP0ht",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<font color=\"magenta\">**For you to ponder:**</font> How does the plot of cross-validation errors for different `C` look? Is there clearly a trough in which the returned value of `C` falls? Does the plot provide some reassurance that the choice is reasonable?"
      ]
    },
    {
      "metadata": {
        "id": "jT3_YcthP0ht",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}